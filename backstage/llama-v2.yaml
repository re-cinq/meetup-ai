apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  # Renamed slightly to reflect the change
  name: embedded-llama-gke-deployment
  title: Embedded Llama Deployment on GKE
  description: Meta-llama/Llama-2-7b-chat-hf on GKE using an embedded manifest.
  tags:
    - llama
    - gke
    - kubernetes
    - tgi
    - ml
spec:
  owner: ai-infra-team # Adjust owner as needed
  type: service

  parameters:
    - title: Deployment Target
      required:
        - namespace
        # Consider adding cluster selection if multiple clusters are configured
        # - clusterName
      properties:
        namespace:
          title: Kubernetes Namespace
          type: string
          description: Target namespace on GKE for the Llama deployment (must exist and contain 'hf-secret').
          ui:field: KubernetesNamespacePicker # Optional: Use specific UI component if available
          ui:options:
            allowedKinds: ['Namespace']
        # clusterName:
        #   title: Target Cluster
        #   type: string
        #   description: Name of the GKE cluster to deploy to.
        #   ui:field: KubernetesClusterPicker # Optional: Use specific UI component

  steps:
    # Step 1: Apply the embedded manifest directly to the target cluster
    # This requires the Backstage backend to have appropriate Kubernetes credentials.
    - id: apply-manifest
      name: Apply Embedded Manifest to GKE Cluster
      action: kubernetes:apply
      input:
        # Use the 'manifest' input with the YAML content directly embedded
        manifest: |
          # --- Start of Embedded Kubernetes Manifest ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            # Backstage templating will correctly substitute the namespace here
            name: llama-deployment-${{ parameters.namespace }}
            namespace: "${{ parameters.namespace }}"
            labels:
              app: llama-app
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: llama-app
            template:
              metadata:
                labels:
                  app: llama-app
              spec:
                nodeSelector:
                  cloud.google.com/gke-accelerator: nvidia-tesla-t4
                containers:
                  - name: llama-container
                    image: ghcr.io/huggingface/text-generation-inference:latest
                    ports:
                      - containerPort: 8080
                    env:
                      - name: MODEL_ID
                        value: meta-llama/Llama-2-7b-chat-hf
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: hf-secret
                            key: hf_token
                    resources:
                      limits:
                        nvidia.com/gpu: 1
          # --- End of Embedded Kubernetes Manifest ---
        # If you added a clusterName parameter, you might need to specify it:
        # clusterRef: ${{ parameters.clusterName }} # Adjust based on your K8s plugin config

    # Optional but recommended: Wait for the deployment to become available
    - id: wait-for-deployment
      name: Wait for Deployment to Become Available
      action: kubernetes:wait # Requires Kubernetes plugin >= v0.9.0
      input:
        apiVersion: apps/v1 # Explicitly state apiVersion for Deployment
        kind: Deployment
        # The name must match the metadata.name defined in the embedded manifest
        name: llama-deployment-${{ parameters.namespace }}
        namespace: ${{ parameters.namespace }}
        for: condition=Available=true # Wait for the Available condition
        timeoutSeconds: 300 # Wait up to 5 minutes

  output:
    links:
      - title: TGI Documentation
        url: https://huggingface.co/docs/text-generation-inference/index
      - title: Deployed Model (Example - Check Service/Ingress)
        icon: dashboard
        # Note: The manifest MUST create a Service/Ingress for this link to be meaningful.
        # This URL structure is a guess and depends entirely on how you expose the service.
        url: http://llama-service.${{ parameters.namespace }}.svc.cluster.local:8080 # Example internal URL