apiVersion: apps/v1
kind: Deployment
metadata:
  name: bert-model
  namespace: default
  labels:
    app: bert-model
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bert-model
  template:
    metadata:
      labels:
        app: bert-model
    spec:
      containers:
      - name: bert-model
        image: huggingface/transformers-pytorch-gpu:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_NAME
          value: "bert-base-uncased"
        - name: TRANSFORMERS_CACHE
          value: "/cache"
        command: ["/bin/bash", "-c"]
        args:
          - |
            pip install transformers[torch] fastapi uvicorn
            python -c "
            from transformers import AutoModel, AutoTokenizer
            import torch
            import uvicorn
            from fastapi import FastAPI, HTTPException
            import numpy as np
            
            app = FastAPI()
            tokenizer = AutoTokenizer.from_pretrained('$MODEL_NAME')
            model = AutoModel.from_pretrained('$MODEL_NAME').to('cuda')
            
            @app.get('/health')
            def health():
                return {'status': 'ok'}
                
            @app.post('/predict')
            def predict(text: str):
                inputs = tokenizer(text, return_tensors='pt').to('cuda')
                with torch.no_grad():
                    outputs = model(**inputs)
                return {'embeddings': outputs.last_hidden_state[:,0,:].cpu().numpy().tolist()}
                
            uvicorn.run(app, host='0.0.0.0', port=8000)
            "
      nodeSelector:
        gpu-node: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"