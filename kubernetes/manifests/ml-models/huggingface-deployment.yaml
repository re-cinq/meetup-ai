apiVersion: apps/v1
kind: Deployment
metadata:
  name: bert-model
  namespace: ml-models
  labels:
    app: bert-model
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bert-model
  template:
    metadata:
      labels:
        app: bert-model
    spec:
      containers:
      - name: bert-model
        image: huggingface/transformers-pytorch-gpu:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_NAME
          value: "bert-base-uncased"
        command: ["/bin/bash"]
        args:
          - "-c"
          - |
            pip install -q transformers torch datasets fastapi uvicorn
            # Use python3 instead of python
            python3 -c "
            import torch
            print('CUDA available:', torch.cuda.is_available())
            print('CUDA device count:', torch.cuda.device_count())
            if torch.cuda.is_available():
                print('CUDA device name:', torch.cuda.get_device_name(0))
            
            from transformers import AutoTokenizer, AutoModel
            print('Loading tokenizer...')
            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            print('Loading model...')
            model = AutoModel.from_pretrained('bert-base-uncased')
            if torch.cuda.is_available():
                print('Moving model to GPU...')
                model = model.to('cuda')
            
            print('Model loaded successfully!')
            print('Running on device:', next(model.parameters()).device)
            
            # Keep container alive
            import time
            while True:
                print('Model running...', flush=True)
                time.sleep(60)
            "
      nodeSelector:
        gpu-node: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"